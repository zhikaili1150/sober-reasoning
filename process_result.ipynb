{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27430bab",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1199c2cd",
   "metadata": {},
   "source": [
    "## Sober Leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6d83f6",
   "metadata": {},
   "source": [
    "`input`: æŸä¸ªmodelçš„result folder path\n",
    "\n",
    "`output`: leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6bf951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def summarize_leaderboard_results(result_path, tasks, metric=\"extractive_match\", output=\"avg.csv\"):\n",
    "    \"\"\"\n",
    "    æ‰«æ result_path ä¸‹æ‰€æœ‰ jsonï¼Œè®¡ç®—æ¯ä¸ª task çš„ mean / stdï¼Œä»¥åŠ mean of meansï¼Œ\n",
    "    å¹¶ä¿å­˜åˆ° CSVã€‚\n",
    "\n",
    "    Args:\n",
    "        result_path (str / Path): JSON æ–‡ä»¶æ‰€åœ¨ç›®å½•\n",
    "        tasks (list[str]): ä»»åŠ¡åç§°åˆ—è¡¨ï¼Œå¦‚ [\"aime24\", \"aime25\", ...]\n",
    "        metric (str): è¦æå–çš„æŒ‡æ ‡ key (é»˜è®¤ extractive_match)\n",
    "        output (str): è¾“å‡º CSV æ–‡ä»¶å (åœ¨ result_path å†…è¾“å‡º)\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: æ±‡æ€»åŽçš„ DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    result_path = Path(result_path)\n",
    "    task_values = {task: [] for task in tasks}\n",
    "\n",
    "    # éåŽ†æ–‡ä»¶å¤¹\n",
    "    for root, dirs, files in os.walk(result_path):\n",
    "        for f in files:\n",
    "            if f.endswith(\".json\"):\n",
    "                fpath = os.path.join(root, f)\n",
    "                with open(fpath, \"r\") as fp:\n",
    "                    data = json.load(fp)\n",
    "\n",
    "                for task in tasks:\n",
    "                    key = f\"custom|{task}|0\"\n",
    "                    if key in data.get(\"results\", {}):\n",
    "                        value = data[\"results\"][key].get(metric, None)\n",
    "                        task_values[task].append(value)\n",
    "\n",
    "    # mean / std èšåˆ\n",
    "    mean_dict = {}\n",
    "    std_dict = {}\n",
    "    mean_values = []\n",
    "\n",
    "    for task, values in task_values.items():\n",
    "        arr = np.array(values, dtype=np.float64)\n",
    "        task_mean = np.nanmean(arr) * 100\n",
    "        task_std = np.nanstd(arr) * 100\n",
    "\n",
    "        mean_dict[task] = round(task_mean, 1)\n",
    "        std_dict[f\"{task}_std\"] = round(task_std, 1)\n",
    "        mean_values.append(task_mean)\n",
    "\n",
    "    # Mean of Means\n",
    "    mean_of_means = round(np.nanmean(mean_values), 1)\n",
    "    result = {**mean_dict, \"Avg.\": mean_of_means, **std_dict}\n",
    "\n",
    "    # åˆ—é¡ºåº\n",
    "    mean_cols = tasks\n",
    "    avg_col = [\"Avg.\"]\n",
    "    std_cols = [f\"{task}_std\" for task in tasks]\n",
    "    ordered_cols = mean_cols + avg_col + std_cols\n",
    "\n",
    "    df = pd.DataFrame([result], columns=ordered_cols)\n",
    "\n",
    "    # è¾“å‡º CSV\n",
    "    output_path = result_path / output\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(df)\n",
    "    print(f\"âœ… å·²ä¿å­˜åˆ° {output_path}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df = summarize_leaderboard_results(\n",
    "    result_path=\"result/ties_accfmt\",\n",
    "    tasks=[\"aime24\", \"aime25\", \"amc23\", \"math_500\", \"minerva\", \"olympiadbench\"],\n",
    "    metric=\"extractive_match\",\n",
    "    output=\"leaderboard.csv\"\n",
    ")\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba81d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def collect_all_results(root_path, tasks, metric=\"extractive_match\", output=\"all_models.csv\"):\n",
    "    \"\"\"\n",
    "    éåŽ† root_path ä¸‹æ‰€æœ‰å­æ–‡ä»¶å¤¹ï¼Œå¯¹æ¯ä¸ªå­æ–‡ä»¶å¤¹è°ƒç”¨ summarize_leaderboard_resultsï¼Œ\n",
    "    æ”¶é›†å¤šä¸ª DataFrameï¼Œå¹¶åˆå¹¶æˆä¸€ä¸ªæ€» CSVã€‚\n",
    "\n",
    "    Args:\n",
    "        root_path (str / Path): åŒ…å«å¤šä¸ª experiment å­ç›®å½•çš„æ ¹ç›®å½•\n",
    "        tasks (list[str]): benchmark list\n",
    "        metric (str): æå–çš„æŒ‡æ ‡\n",
    "        output (str): è¾“å‡ºæ–‡ä»¶å\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: åˆå¹¶åŽçš„ DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    root_path = Path(root_path)\n",
    "    all_dfs = []\n",
    "\n",
    "    for subdir in sorted(root_path.iterdir()):\n",
    "        if subdir.is_dir():\n",
    "            print(f\"ðŸ“‚ Processing model: {subdir.name}\")\n",
    "            df = summarize_leaderboard_results(\n",
    "                result_path=subdir,\n",
    "                tasks=tasks,\n",
    "                metric=metric,\n",
    "                output=\"leaderboard.csv\"\n",
    "            )\n",
    "            df[\"model\"] = subdir.name  # åŠ ä¸Šæ¨¡åž‹æ ‡è¯†\n",
    "            all_dfs.append(df)\n",
    "\n",
    "    # åˆå¹¶\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    # è¾“å‡º CSV\n",
    "    output_path = root_path / output\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(\"\\nâœ… All experiments summarized.\")\n",
    "    print(f\"ðŸ“„ Final CSV saved to: {output_path}\")\n",
    "    print(final_df)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "# -------- è¿è¡Œ --------\n",
    "collect_all_results(\n",
    "    root_path=\"/Users/zhikaili/workspace/sober-reasoning/result/ties_accfmt/results\",     # è¿™ä¸ªç›®å½•é‡Œé¢åŒ…å«å¤šä¸ªå­ç›®å½•ï¼Œæ¯ä¸ªå­ç›®å½•å¯¹åº”ä¸€ä¸ª experiment\n",
    "    tasks=[\"aime24\", \"aime25\", \"amc23\", \"math_500\", \"minerva\", \"olympiadbench\"],\n",
    "    metric=\"extractive_match\",\n",
    "    output=\"all_models.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7258ec29",
   "metadata": {},
   "source": [
    "## Plot Eval vs Ratio\n",
    "\n",
    "input: åŒ…å«å¤šä¸ªmodelè¯„ä¼°ç»“æžœçš„folderï¼Œæ–‡ä»¶å¤¹åå­—ä¸­è¦å¸¦æœ‰ratioç”¨äºŽè§£æž\n",
    "ouput: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3458505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# ===============================\n",
    "# Data\n",
    "# ===============================\n",
    "\n",
    "ratios = [\n",
    "    \"0:1\",\n",
    "    \"0.1:0.9\",\n",
    "    \"0.2:0.8\",\n",
    "    \"0.3:0.7\",\n",
    "    \"0.4:0.6\",\n",
    "    \"0.5:0.5\",\n",
    "    \"0.6:0.4\",\n",
    "    \"0.7:0.3\",\n",
    "    \"0.8:0.2\",\n",
    "    \"0.9:0.1\",\n",
    "    \"1:0\",\n",
    "]\n",
    "\n",
    "# Accuracy + Format\n",
    "acc_fmt = [48.9, 49.0, 49.4, 48.6, 49.2, 49.5, 49.7, 50.5, 48.8, 48.7, 49.1]\n",
    "\n",
    "# Cosine + Format\n",
    "cos_fmt = [48.9, 50.1, 48.3, 48.2, 48.6, 49.5, 50.0, 49.9, 49.0, 48.9, 49.5]\n",
    "\n",
    "# Baseline values\n",
    "baseline_dra = 49.3\n",
    "baseline_rs = 49.0\n",
    "baseline_base = 48.4\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Plot\n",
    "# ===============================\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# ä¸¤æ¡ main æ›²çº¿\n",
    "plt.plot(ratios, acc_fmt, marker=\"o\", linewidth=2, label=\"Accuracy + Format\")\n",
    "plt.plot(ratios, cos_fmt, marker=\"o\", linewidth=2, label=\"Cosine + Format\")\n",
    "\n",
    "# Baseline\n",
    "plt.axhline(\n",
    "    baseline_dra,\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    color=\"red\",\n",
    "    label=f\"Baseline: DRA-GRPO ({baseline_dra})\",\n",
    ")\n",
    "\n",
    "plt.axhline(\n",
    "    baseline_rs,\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    color=\"purple\",\n",
    "    label=f\"Baseline: OPEN-RS ({baseline_rs})\",\n",
    ")\n",
    "\n",
    "plt.axhline(\n",
    "    baseline_base,\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    color=\"gray\",\n",
    "    label=f\"Baseline: Base Model ({baseline_base})\",\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "#  Expert æ ‡ç­¾æ ‡æ³¨\n",
    "# ===============================\n",
    "\n",
    "# Format Expert â†’ ratio = 0:1\n",
    "plt.text(0, acc_fmt[0], \"Format Expert\", fontsize=12, color=\"black\")\n",
    "\n",
    "# Accuracy Expert â†’ ratio = 1:0ï¼ˆåœ¨ acc+format æ›²çº¿ï¼‰\n",
    "plt.text(10, acc_fmt[-1], \"Accuracy Expert\", fontsize=12, color=\"black\")\n",
    "\n",
    "# Cosine Expert â†’ ratio = 1:0ï¼ˆåœ¨ cosine+format æ›²çº¿ï¼‰\n",
    "plt.text(10, cos_fmt[-1], \"Cosine Expert\", fontsize=12, color=\"black\")\n",
    "\n",
    "\n",
    "plt.title(\"Evaluation Performance Across Ratio\", fontsize=18)\n",
    "plt.xlabel(\"Accuracy/Cosine : Format Ratio\", fontsize=14)\n",
    "plt.ylabel(\"Average Accuracy\", fontsize=14)\n",
    "\n",
    "plt.xticks(rotation=30)\n",
    "plt.grid(alpha=0.4, linestyle=\"--\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1c486e",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "æ”¶é›†æ‰€æœ‰lrï¼Œæ‰€æœ‰stepçš„validationç»“æžœ\n",
    "ç”»å‡ºæŠ˜çº¿å›¾ï¼Œæ¨ªåæ ‡æ˜¯stepï¼Œçºµåæ ‡æ˜¯val accï¼Œæ¯æ¡æŠ˜çº¿ä»£è¡¨ä¸€ä¸ªlr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3964fd",
   "metadata": {},
   "source": [
    "## Collect Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55dc9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def find_json_files(path):\n",
    "    \"\"\"é€’å½’æŸ¥æ‰¾ path ä¸‹çš„æ‰€æœ‰ .json æ–‡ä»¶\"\"\"\n",
    "    json_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for f in files:\n",
    "            if f.endswith(\".json\"):\n",
    "                json_files.append(os.path.join(root, f))\n",
    "    return json_files\n",
    "\n",
    "\n",
    "def summarize_results_to_csv(\n",
    "    root_dir,\n",
    "    tasks=[\"aime24\", \"aime25\", \"amc23\", \"math_500\", \"minerva\", \"olympiadbench\"],\n",
    "    metrics=[\"extractive_match\"],\n",
    "    output=\"results_summary.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    æ±‡æ€» JSON -> CSV\n",
    "\n",
    "    å‚æ•°:\n",
    "    - root_dir: JSON æœç´¢çš„æ ¹ç›®å½•\n",
    "    - tasks:    ä»»åŠ¡åç§°åˆ—è¡¨\n",
    "    - metrics:  æŒ‡æ ‡ key åˆ—è¡¨\n",
    "    - output:   è¾“å‡º CSV æ–‡ä»¶åç§°\n",
    "\n",
    "    è¿”å›ž:\n",
    "    - df (DataFrame): æ±‡æ€»ç»“æžœ\n",
    "    \"\"\"\n",
    "\n",
    "    records = {}\n",
    "\n",
    "    # æ‰¾å…¨éƒ¨ JSON æ–‡ä»¶\n",
    "    json_files = find_json_files(root_dir)\n",
    "    print(f\"ðŸ” å…±æ‰¾åˆ° {len(json_files)} ä¸ª JSON æ–‡ä»¶\")\n",
    "\n",
    "    for fpath in json_files:\n",
    "        with open(fpath, \"r\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ æ— æ³•è¯»å– {fpath}: {e}\")\n",
    "                continue\n",
    "\n",
    "        model = os.path.relpath(fpath, root_dir)\n",
    "\n",
    "        for task in tasks:\n",
    "            for metric in metrics:\n",
    "                key = f\"{task}_{metric}\"\n",
    "                try:\n",
    "                    value = data[\"results\"][f\"custom|{task}|0\"][metric]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "                if model not in records:\n",
    "                    records[model] = {}\n",
    "                if key not in records[model]:\n",
    "                    records[model][key] = []\n",
    "                records[model][key].append(value)\n",
    "\n",
    "    # å°† list è½¬ä¸ºå¹³å‡å€¼\n",
    "    for model, vals in records.items():\n",
    "        for key in vals:\n",
    "            if isinstance(vals[key], list):\n",
    "                records[model][key] = float(np.mean(vals[key]))\n",
    "\n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    rows = []\n",
    "    for model, vals in records.items():\n",
    "        row = {\"model\": model}\n",
    "        for task in tasks:\n",
    "            for metric in metrics:\n",
    "                row[f\"{task}_{metric}\"] = vals.get(f\"{task}_{metric}\", None)\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # æ¯ä¸ª metric å•ç‹¬æ±‚å¹³å‡\n",
    "    for metric in metrics:\n",
    "        cols = [f\"{task}_{metric}\" for task in tasks if f\"{task}_{metric}\" in df.columns]\n",
    "        if cols:\n",
    "            df[f\"avg_{metric}\"] = df[cols].mean(axis=1, skipna=True)\n",
    "\n",
    "    # extractive_match Ã—100\n",
    "    for col in df.columns:\n",
    "        if \"extractive_match\" in col:\n",
    "            df[col] = df[col].apply(lambda x: round(x * 100, 1) if pd.notnull(x) else x)\n",
    "\n",
    "    # ä¿å­˜ CSV\n",
    "    csv_path = os.path.join(root_dir, output)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"âœ… å·²ä¿å­˜ {csv_path}\")\n",
    "    print(f\"ðŸ“Š å…±æ±‡æ€» {len(df)} æ¡è®°å½•\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df = summarize_results_to_csv(\n",
    "    root_dir=\"/local/scratch/zli2255/workspace/open-r1/experiments/\",\n",
    "    metrics=[\"extractive_match\"],\n",
    "    output=\"summary_extractive.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a724452c",
   "metadata": {},
   "source": [
    "## Parse result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b226606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# CSV æ–‡ä»¶è·¯å¾„\n",
    "csv_path = Path('/Users/zhikaili/workspace/open-r1/result/aime2425_acc_len_1e-06/result.csv')\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "def extract_info(path: str):\n",
    "    \"\"\"\n",
    "    ä»Žè·¯å¾„ä¸­æå– ratio, lr, step\n",
    "    ä¾‹å¦‚ï¼šexperiments_exp_grpo_fft_ckpt_reward_expert_0001_9e-07_checkpoint-50\n",
    "    \"\"\"\n",
    "    match = re.search(\n",
    "        r'_([0-9]+)_([\\d.eE+-]+)_checkpoint-(\\d+)',\n",
    "        path\n",
    "    )\n",
    "    if match:\n",
    "        ratio = match.group(1)\n",
    "        lr = match.group(2)\n",
    "        step = match.group(3)\n",
    "        return ratio, lr, step\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "# æå– ratioã€lrã€step\n",
    "df[['ratio', 'lr', 'step']] = df['model'].apply(\n",
    "    lambda x: pd.Series(extract_info(x))\n",
    ")\n",
    "\n",
    "# è½¬æ¢ç±»åž‹\n",
    "df['lr'] = df['lr'].astype(float)\n",
    "df['step'] = df['step'].astype(int)\n",
    "\n",
    "# ä¿å­˜ä¿®æ”¹åŽçš„ CSV\n",
    "df.to_csv(csv_path.parent / 'parsed_result.csv', index=False)\n",
    "print(f\"âœ… Updated CSV saved to {csv_path.parent / 'parsed_result.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d996903",
   "metadata": {},
   "source": [
    "## Aggregate all benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f995974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†ç©ºå€¼ NaN è½¬ä¸º 0\n",
    "# df_filled = df.fillna(0)\n",
    "\n",
    "# åˆ é™¤ object ç±»åž‹çš„åˆ—ï¼ˆæ¯”å¦‚å­—ç¬¦ä¸²åˆ—ï¼‰\n",
    "df_numeric = df.drop(columns=['model'])\n",
    "\n",
    "# å¯¹ ratio, lr, step åˆ†ç»„å–å¹³å‡\n",
    "df_agg = df_numeric.groupby(['ratio', 'lr', 'step'], as_index=False).mean()\n",
    "\n",
    "# ä¿å­˜ç»“æžœ\n",
    "output_path = csv_path.parent / 'aggregated_result.csv'\n",
    "df_agg.to_csv(output_path, index=False)\n",
    "print(f\"âœ… Aggregated CSV saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d003652",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# è¯»å– CSV\n",
    "df = pd.read_csv(output_path, dtype={'ratio': str})\n",
    "\n",
    "df['ratio'] = df['ratio'].astype(str)\n",
    "\n",
    "# èŽ·å–æ‰€æœ‰ learning rates\n",
    "keep_lrs = [1e-6]\n",
    "df = df[df['lr'].isin(keep_lrs)]\n",
    "learningrates = df['lr'].unique()\n",
    "learningrates.sort()\n",
    "\n",
    "# æŽ’åº step\n",
    "df = df.sort_values(by='step')\n",
    "\n",
    "\n",
    "# ç»˜å›¾å‡½æ•°\n",
    "def plot_metric(sub_df, metric, ylabel, title, ratio_val):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    for lr in learningrates:\n",
    "        lr_df = sub_df[sub_df['lr'] == lr]\n",
    "\n",
    "        if metric == 'avg_extractive_match':\n",
    "            steps = lr_df['step'].tolist()\n",
    "            values = lr_df[metric].tolist()\n",
    "        else:\n",
    "            steps = lr_df['step'].tolist()\n",
    "            values = lr_df[metric].tolist()\n",
    "\n",
    "        plt.plot(steps, values, label=lr)\n",
    "\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(f\"{title} (ratio={ratio_val})\")\n",
    "    plt.legend(title='Learning Rate')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# å¯¹æ¯ä¸ª ratio ç»˜å›¾\n",
    "for ratio_val, sub_df in df.groupby('ratio'):\n",
    "    plot_metric(sub_df, 'avg_extractive_match', 'Avg@3', 'Validation Avg@3 vs Step', ratio_val)\n",
    "    plot_metric(sub_df, 'avg_mean_token_length', 'Mean Output Token Length', 'Mean Output Token Length vs Step', ratio_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1069a80a",
   "metadata": {},
   "source": [
    "## Validation Merged Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a85008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_strings(n_points=30, total=1):\n",
    "    result = []\n",
    "    for i in range(n_points):\n",
    "        left = round(total * i / (n_points - 1), 3)\n",
    "        right = round(total - left, 3)\n",
    "        result.append(f'\"{0.000:.3f} {left:.3f} {0.000:.3f} {right:.3f}\"')\n",
    "    return result\n",
    "\n",
    "strings = generate_strings(30)\n",
    "for s in strings:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8369e6c7",
   "metadata": {},
   "source": [
    "parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b0c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# è¯»å– CSV\n",
    "result_csv = Path(\"val_cosfmt.csv\")\n",
    "df = pd.read_csv(result_csv)\n",
    "\n",
    "output_path = result_csv.parent / \"grouped_result.csv\"\n",
    "\n",
    "\n",
    "df['ratio'] = df['model'].str.extract(r'_([0-9.]+)/').astype(float)\n",
    "\n",
    "\n",
    "# æŒ‰ ratio åˆ†ç»„è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®\n",
    "group_stats = df.groupby('ratio').agg(\n",
    "    math_validation_mean_token_length_mean=('math_validation_mean_token_length', 'mean'),\n",
    "    math_validation_mean_token_length_std=('math_validation_mean_token_length', 'std'),\n",
    "    math_validation_extractive_match_mean=('math_validation_extractive_match', 'mean'),\n",
    "    math_validation_extractive_match_std=('math_validation_extractive_match', 'std')\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "group_stats.to_csv(output_path, index=False)\n",
    "\n",
    "group_stats.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbd1ab3",
   "metadata": {},
   "source": [
    "## Plot Val vs Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20da80d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(output_path)  # æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„\n",
    "\n",
    "# å›¾ 1: mean_token_length\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(df['ratio'], df['math_validation_mean_token_length_mean'], marker='o')\n",
    "plt.xlabel('Ratio')\n",
    "plt.ylabel('Mean Token Length')\n",
    "plt.title('Mean Token Length vs Merging Ratio')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# å›¾: math_validation_extractive_match_mean + std (å¡«å……å¸¦)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(df['ratio'], df['math_validation_extractive_match_mean'], 'o-', color='blue', label='Mean')\n",
    "plt.fill_between(\n",
    "    df['ratio'],\n",
    "    df['math_validation_extractive_match_mean'] - df['math_validation_extractive_match_std'],\n",
    "    df['math_validation_extractive_match_mean'] + df['math_validation_extractive_match_std'],\n",
    "    color='blue',\n",
    "    alpha=0.1,  # åŠé€æ˜Ž\n",
    "    label='Std Dev'\n",
    ")\n",
    "plt.xlabel('Ratio')\n",
    "plt.ylabel('Avg@3')\n",
    "plt.title('Validation Performance vs Merging Ratio')\n",
    "plt.axhline(y=44, color='red', linestyle='--', linewidth=2, label='Base Model')\n",
    "\n",
    "plt.text(0.1, df['math_validation_extractive_match_mean'].iloc[-1], \"Format Expert\",\n",
    "         ha='center', va='bottom', color='black', fontsize=10)\n",
    "plt.text(0.9, df['math_validation_extractive_match_mean'].iloc[0], \"Cosine Expert\",\n",
    "         ha='center', va='bottom', color='black', fontsize=10)\n",
    "\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c64dd08",
   "metadata": {},
   "source": [
    "# Detail -> Acc, Token Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63623470",
   "metadata": {},
   "source": [
    "ä¸€ä¸ªmodelä¼šè¿›è¡Œå¤šæ¬¡validationï¼Œæ¯æ¬¡validationä¼šæœ‰ä¸€ä¸ªå¯¹åº”çš„.parquetæ–‡ä»¶ï¼Œå› æ­¤å¯¹äºŽä¸€ä¸ªmodelçš„ä¸€æ¬¡validationï¼Œéœ€è¦ä»Žè¿™äº›parquetæ–‡ä»¶ä¸­æå–accå’Œlenä¿¡æ¯ã€‚\n",
    "è¿™é‡Œçš„ä¸€ä¸ªmodelå¯ä»¥æ˜¯æŸä¸ªstepçš„checkpointï¼Œæˆ‘ä»¬éœ€è¦æ”¶é›†æ‰€æœ‰checkpointçš„accå’Œlenï¼Œä»Žè€Œæž„å»ºpareto frontã€‚\n",
    "\n",
    "\n",
    "\n",
    "- `fix` 10ä¸ªratio10ä¸ªstepçš„checkpointéƒ½åœ¨aime24ä¸Šè¿›è¡Œåæ¬¡éªŒè¯ -> 10 * 10 * (acc, len) -> pareto\n",
    "- `merge` åŒä¸Š\n",
    "- `dynamic` mix **1ä¸ªratio**10ä¸ªstepçš„checkpointéƒ½åœ¨aime24ä¸Šè¿›è¡Œåæ¬¡éªŒè¯?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6d44a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def analyze_parquet_folder(\n",
    "    folder_path: str,\n",
    "    task: str,\n",
    "    model_name: str = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    save_csv: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    é€’å½’éåŽ†æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰ .parquet æ–‡ä»¶ï¼Œ\n",
    "    è®¡ç®—å¹³å‡ token length ä¸Ž extractive_matchã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "        folder_path (str): è¦åˆ†æžçš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    "        model_name (str): ç”¨äºŽåˆ†è¯çš„æ¨¡åž‹åç§°\n",
    "        save_csv (bool): æ˜¯å¦ä¿å­˜è¯¦ç»†ç»“æžœ CSV (é»˜è®¤ False)\n",
    "\n",
    "    è¿”å›ž:\n",
    "        dict: {\"avg_token_length\": float, \"avg_extractive_match\": float}\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # === æ”¶é›†æ‰€æœ‰ parquet æ–‡ä»¶ ===\n",
    "    parquet_files = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for f in files:\n",
    "            if f.endswith(\".parquet\") and task in f:\n",
    "                parquet_files.append(os.path.join(root, f))\n",
    "\n",
    "    if not parquet_files:\n",
    "        print(f\"âš ï¸ No parquet files found under {folder_path}\")\n",
    "        return {\"avg_token_length\": np.nan, \"avg_extractive_match\": np.nan}\n",
    "\n",
    "    print(f\"ðŸ” Found {len(parquet_files)} parquet files under {folder_path}\")\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for parquet_path in parquet_files:\n",
    "        try:\n",
    "            details = load_dataset(\"parquet\", data_files=parquet_path, split=\"train\")\n",
    "            token_lengths, extractive_matches = [], []\n",
    "\n",
    "            for item in details:\n",
    "                preds = item.get(\"predictions\", [])\n",
    "                if preds and isinstance(preds[0], str):\n",
    "                    tokens = tokenizer.encode(preds[0])\n",
    "                    token_lengths.append(len(tokens))\n",
    "\n",
    "                metrics = item.get(\"metrics\", {})\n",
    "                if isinstance(metrics, dict) and \"extractive_match\" in metrics:\n",
    "                    extractive_matches.append(metrics[\"extractive_match\"])\n",
    "\n",
    "            if token_lengths or extractive_matches:\n",
    "                avg_len = np.mean(token_lengths) if token_lengths else np.nan\n",
    "                avg_em = np.mean(extractive_matches) if extractive_matches else np.nan\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"file\": os.path.basename(parquet_path),\n",
    "                        \"avg_token_length\": avg_len,\n",
    "                        \"avg_extractive_match\": avg_em,\n",
    "                    }\n",
    "                )\n",
    "                print(\n",
    "                    f\"ðŸ“„ {os.path.basename(parquet_path)} â†’ tokens={avg_len:.1f}, extractive_match={avg_em:.3f}\"\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to process {parquet_path}: {e}\")\n",
    "\n",
    "    if not records:\n",
    "        print(\"âš ï¸ No valid data found.\")\n",
    "        return {\"avg_token_length\": np.nan, \"avg_extractive_match\": np.nan}\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    avg_len = df[\"avg_token_length\"].mean()\n",
    "    avg_em = df[\"avg_extractive_match\"].mean()\n",
    "\n",
    "    print(f\"\\nâœ… Folder summary â†’ tokens={avg_len:.1f}, extractive_match={avg_em:.3f}\")\n",
    "\n",
    "    if save_csv:\n",
    "        output_csv = os.path.join(folder_path, \"aggregated_metrics.csv\")\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"ðŸ’¾ Detailed results saved to: {output_csv}\")\n",
    "\n",
    "    return {\"avg_token_length\": avg_len, \"avg_extractive_match\": avg_em}\n",
    "\n",
    "\n",
    "# === å‚æ•°è®¾ç½® ===\n",
    "parent_dir = \"/Users/zhikaili/workspace/open-r1/result/aime25_accfmt/details\"\n",
    "task = \"aime25\"\n",
    "output_csv = os.path.join(parent_dir, \"summary.csv\")\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "\n",
    "records = []\n",
    "\n",
    "# === éåŽ†å­æ–‡ä»¶å¤¹ ===\n",
    "for folder_name in sorted(os.listdir(parent_dir)):\n",
    "    folder_path = os.path.join(parent_dir, folder_name)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"ðŸ“‚ Processing folder: {folder_name}\")\n",
    "    try:\n",
    "        result = analyze_parquet_folder(folder_path, task, model_name)\n",
    "        result[\"folder_name\"] = folder_name\n",
    "        records.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to process {folder_name}: {e}\")\n",
    "\n",
    "# === æ±‡æ€»ä¿å­˜ ===\n",
    "if records:\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\nâœ… Summary saved to {output_csv}\")\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"âš ï¸ No valid results found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973a6804",
   "metadata": {},
   "source": [
    "## CSV:(step, acc, len) -> Pareto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a76a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === è¯»å– CSV ===\n",
    "df = pd.read_csv(\"result/aime2425_acc_len_1e-06/acclenstep.csv\")\n",
    "\n",
    "\n",
    "# === Step 1: æ•£ç‚¹å›¾ ===\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(df['acc'], df['len'], c='blue', label='all points')\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Token length\")\n",
    "plt.title(\"Scatter plot of (acc, len)\")\n",
    "plt.grid(True)\n",
    "\n",
    "# === Step 2: æ‰¾ Pareto front ===\n",
    "points = df[['acc','len']].values\n",
    "\n",
    "def pareto_front(points):\n",
    "    \"\"\"è¿”å›žå¸ƒå°” maskï¼ŒTrue è¡¨ç¤ºåœ¨ Pareto å‰æ²¿\"\"\"\n",
    "    N = points.shape[0]\n",
    "    mask = np.ones(N, dtype=bool)\n",
    "    for i in range(N):\n",
    "        if not mask[i]:\n",
    "            continue\n",
    "        for j in range(N):\n",
    "            if i == j:\n",
    "                continue\n",
    "            # j dominates i if acc_j >= acc_i and len_j <= len_i and at least one strict\n",
    "            if (points[j,0] >= points[i,0] and points[j,1] <= points[i,1]) and (\n",
    "                points[j,0] > points[i,0] or points[j,1] < points[i,1]\n",
    "            ):\n",
    "                mask[i] = False\n",
    "                break\n",
    "    return mask\n",
    "\n",
    "mask = pareto_front(points)\n",
    "pareto_points = points[mask]\n",
    "\n",
    "# === Step 3: ç»˜åˆ¶ Pareto front ===\n",
    "plt.scatter(pareto_points[:,0], pareto_points[:,1], c='red', label='Pareto front')\n",
    "plt.plot(pareto_points[:,0], pareto_points[:,1], c='red', linestyle='--')  # è¿žçº¿\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
